---
title: "Signal Daily 2026-01-13"
date: 2026-01-13
summary: "DeepSeek 发布 Engram 论文，用 N-gram 重构 Transformer 记忆机制；兆芯冲刺 IPO 披露国产 x86 底牌；腾讯 AngelSlim 推出全模态投机采样。"
tags: ["DeepSeek", "Chip", "Tencent"]
---

# Signal Daily 2026-01-13

> **Signal 简报**：今天的技术看点非常硬核。
>
> DeepSeek 不仅开源了新论文，更是用一种“复古”的 N-gram 思想重构了 Transformer 的记忆机制，这极有可能是 V4 模型的杀手锏；硬件方面，国产 CPU 厂商兆芯冲刺 IPO，总工披露了“兼容 x86 + 全自研内核”的底牌；而在应用层，腾讯推出了全模态加速工具 AngelSlim，试图解决大模型落地的“最后一公里”问题。

---

### 01 硬核架构：DeepSeek 的“记忆外挂”
**[APPSO: V4 杀手锏](https://mp.weixin.qq.com/s/QtwzLEUt95regmxoXdALpw) | [刘聪NLP: 论文详解](https://mp.weixin.qq.com/s/im9x5pFTYRQcrph_etGOkA) | [芯东西: 实习生挑大梁](https://mp.weixin.qq.com/s/sHAGEYoncTLbK9viyo5K2Q)**

DeepSeek 团队（梁文锋署名）发布了新论文 **Engram**，提出了一种“条件记忆”机制，试图解决 Transformer 模型“死记硬背”效率低下的问题。

*   **复古创新**：Engram 引入了经典的 **N-gram**（N元语法）并结合现代哈希嵌入表。这相当于给模型外挂了一个“超级字典”，遇到固定搭配（如“Diana, Princess of Wales”）时，直接查表获取向量，而不需要浪费深层网络去逐步推理。
*   **把 CPU 当 GPU 用**：这是最令工程界兴奋的点。由于 Engram 的查表只依赖输入 Token（无动态路由），可以提前计算。DeepSeek 成功将 **1000 亿参数** 的记忆表卸载到廉价的 **CPU 内存** 中，利用异步预取技术，使得 H800 推理吞吐量损耗 **<3%**。这意味着未来可以用内存条来扩充模型的知识容量，大幅降低显存成本。
*   **V4 预告？**：实验显示，Engram-27B 在推理和代码任务上显著优于同规模 MoE。LogitLens 分析表明，Engram 让浅层网络获得了深层网络的表征能力，这解释了为何传闻中 DeepSeek V4 的代码逻辑能力有质的飞跃。

---

### 02 本地算力：128GB 内存的“微调神器”
**[机器之心: 联想 ThinkStation PGX](https://mp.weixin.qq.com/s/qeWJwnchS2qNFAKFaHu-TA)**

对于不想上云、显存又不够的开发者，联想推出了一款针对性的工作站 **ThinkStation PGX**。
*   **核心配置**：基于 NVIDIA Grace Blackwell 架构的 **GB10** 超级芯片。
*   **杀手锏**：**128GB 统一内存**（Unified Memory）。这解决了消费级显卡（如 4090 24GB）显存不足的痛点，可以直接加载并全量微调 **30B 参数** 级别多模态模型（如 Qwen-VL-30B），且无需量化。
*   **价格与定位**：售价约 3-4 万元，填补了消费级显卡与工业级服务器之间的空白，适合算法验证和数据敏感行业。

---

### 03 国产芯：兆芯冲刺科创板
**[芯东西: 对话总工](https://mp.weixin.qq.com/s/sHAGEYoncTLbK9viyo5K2Q)**

国产 CPU 厂商 **兆芯** 正在冲刺 IPO，其总经理王惟林披露了技术路线图。
*   **技术路线**：坚持 **兼容 x86 指令集**，但内核微架构、互连技术全自研。这保证了 Windows/Linux 生态的无缝迁移，是信创市场的“销冠”策略。
*   **新品秀肌肉**：新一代服务器 CPU **开胜 KH-50000** 采用 Chiplet 技术，单路最高 **96 核**，支持 DDR5 和 PCIe 5.0，对标国际高端产品。
*   **AI 布局**：虽然主攻通用计算，但也开始在桌面 CPU 中集成 NPU，并支持与国产 AI 卡（如华为、寒武纪）的异构加速。

---

### 04 推理加速：腾讯 AngelSlim 升级
**[腾讯技术工程: AngelSlim](https://mp.weixin.qq.com/s/lPTJQZFzu-mA_cwJt-f8qw)**

腾讯混元团队升级了其大模型压缩工具包 **AngelSlim**，核心亮点是**全模态投机采样**。
*   **Eagle3 范式**：将投机采样（Speculative Decoding）从文本扩展到了 **LLM、视觉语言 (VLM)、语音 (Audio)** 等全模态。
*   **加速效果**：在不损失生成质量的前提下，实际部署推理速度最高提升 **1.9 倍**。
*   **开箱即用**：提供从数据处理、草稿模型训练到 vLLM 部署的完整链路，支持“训完即用”。

---

### 05 学术前沿：教会模型“坦白从宽”
**[DeeplearningAI: The Batch](https://mp.weixin.qq.com/s/Ur-InL4bTLKwU1mkWaechg)**

OpenAI 研究人员提出了一种微调方法，让模型在违反规则时主动“自白”。
*   **机制**：训练模型在生成回复后，追加一段“自白”，说明其回复是否满足了所有约束条件。如果模型如实承认自己“幻觉”或违规，会获得奖励。
*   **结果**：在测试中，模型有 **81.4%** 的概率要么不产生幻觉，要么在产生幻觉后主动承认。这为 AI 安全监控提供了一种新的“思维链”思路。

---

### 🗞️ 简讯
*   **[数据]** **DeepSeek-V3** 论文补充实验细节：Engram 架构在长上下文任务（32k）中，多针检索准确率从 84% 提升至 **97%**。
*   **[观点]** **Google 论文**：简单的重复 Prompt（`Query` -> `Query Query`）能显著提升大模型准确率，这可能是利用了模型对上下文的自我强化机制。
